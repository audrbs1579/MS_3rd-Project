{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e72094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1단계 Autoencoder 모델 로드 완료: C:\\Users\\EL040\\Desktop\\MS_3rd-Project\\Project\\models\\base_autoencoder.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EL040\\anaconda3\\envs\\guardian_poc\\lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\EL040\\anaconda3\\envs\\guardian_poc\\lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\EL040\\anaconda3\\envs\\guardian_poc\\lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x07'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# 1-2. Stage 2: Classifier 모델 및 임계값 로드\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(STAGE2_MODEL_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 47\u001b[0m     stage2_data \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m model_clf \u001b[38;5;241m=\u001b[39m stage2_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpipeline\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     49\u001b[0m THRESHOLD \u001b[38;5;241m=\u001b[39m stage2_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '\\x07'."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np # numpy import 추가\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 경로 설정 ---\n",
    "DATA_DIR = r\"C:\\Users\\EL040\\Desktop\\MS_3rd-Project\\Project\\data\"\n",
    "MODEL_DIR = r\"C:\\Users\\EL040\\Desktop\\MS_3rd-Project\\Project\\models\"\n",
    "STAGE1_MODEL_PATH = os.path.join(MODEL_DIR, \"base_autoencoder.pt\")\n",
    "STAGE2_MODEL_PATH = os.path.join(MODEL_DIR, \"stage2_classifier.pkl\")\n",
    "# ---\n",
    "\n",
    "# 1. 모델 로드\n",
    "# 1-1. Stage 1: Autoencoder 모델 구조 정의 (모델 훈련 총정리.txt 기반)\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(45, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 32)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 45)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Autoencoder 모델 로드\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ae = Autoencoder()\n",
    "model_ae.load_state_dict(torch.load(STAGE1_MODEL_PATH, map_location=device))\n",
    "model_ae.to(device)\n",
    "model_ae.eval()\n",
    "print(f\"1단계 Autoencoder 모델 로드 완료: {STAGE1_MODEL_PATH}\")\n",
    "\n",
    "# 1-2. Stage 2: Classifier 모델 및 임계값 로드\n",
    "with open(STAGE2_MODEL_PATH, 'rb') as f:\n",
    "    stage2_data = pickle.load(f)\n",
    "model_clf = stage2_data['pipeline']\n",
    "THRESHOLD = stage2_data['threshold']\n",
    "print(f\"2단계 Classifier 모델 로드 완료: {STAGE2_MODEL_PATH}\")\n",
    "print(f\"적용될 임계값(Threshold): {THRESHOLD}\")\n",
    "\n",
    "# 2. 데이터 로드\n",
    "json_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.json')]\n",
    "all_logs = []\n",
    "for file in tqdm(json_files, desc=\"JSON 파일 로딩 중\"):\n",
    "    with open(os.path.join(DATA_DIR, file), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                all_logs.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                # 가끔 파일 끝에 비어있는 라인이 있을 경우 대비\n",
    "                continue\n",
    "df_raw = pd.DataFrame(all_logs)\n",
    "print(f\"총 {len(df_raw)}개의 로그를 로드했습니다.\")\n",
    "\n",
    "\n",
    "# 3. 전처리 함수 (!!! 사용자가 직접 채워야 하는 부분 !!!)\n",
    "def preprocess_to_features(df):\n",
    "    \"\"\"\n",
    "    원본 데이터프레임을 입력받아 1단계, 2단계 모델에 필요한 피처를 생성합니다.\n",
    "    이 함수는 '02_feature_engineering.ipynb'의 로직을 기반으로 완성해야 합니다.\n",
    "    \n",
    "    Returns:\n",
    "        - df_processed: 2단계 모델용 피처(버킷 등)가 포함된 데이터프레임\n",
    "        - features_45d: 1단계 모델용 45차원 수치형 피처 (numpy array)\n",
    "    \"\"\"\n",
    "    print(\"주의: 전처리 함수(preprocess_to_features)는 사용자의 실제 로직으로 채워져야 합니다.\")\n",
    "    \n",
    "    # --- 예시 코드 (반드시 실제 코드로 교체 필요) ---\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # 2단계 모델용 피처 생성 (예시)\n",
    "    df_processed['actor_login_bucket'] = df['actor'].apply(lambda x: x.get('login', '<EMPTY>')).astype(str).str[0]\n",
    "    df_processed['repo_bucket'] = df['repo'].apply(lambda x: x.get('name', '<EMPTY>')).astype(str).str.split('/').str[0]\n",
    "    df_processed['org_bucket'] = df.get('org', pd.Series(dtype=object)).apply(\n",
    "        lambda x: x.get('login', '<EMPTY>') if isinstance(x, dict) else '<EMPTY>'\n",
    "    ).astype(str)\n",
    "    \n",
    "    # 1단계 모델용 45차원 피처 생성 (예시 - 랜덤 더미 데이터)\n",
    "    # 이 부분에 실제 45차원 피처 생성 로직이 들어가야 합니다.\n",
    "    num_samples = len(df)\n",
    "    features_45d = np.random.rand(num_samples, 45)\n",
    "    # -----------------------------------------\n",
    "    \n",
    "    return df_processed, features_45d\n",
    "\n",
    "df_processed, features_45d = preprocess_to_features(df_raw)\n",
    "\n",
    "\n",
    "# 4. 1단계 추론: Anomaly Score 계산\n",
    "print(\"1단계 추론을 시작합니다 (Anomaly Score 계산)...\")\n",
    "features_tensor = torch.FloatTensor(features_45d).to(device)\n",
    "\n",
    "# 메모리 문제를 방지하기 위해 데이터를 작은 배치로 나누어 처리\n",
    "batch_size = 8192\n",
    "anomaly_scores = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(features_tensor), batch_size), desc=\"Anomaly Score 계산 중\"):\n",
    "        batch = features_tensor[i:i+batch_size]\n",
    "        reconstructions = model_ae(batch)\n",
    "        mse = nn.functional.mse_loss(reconstructions, batch, reduction='none')\n",
    "        batch_scores = torch.mean(mse, dim=1).cpu().numpy()\n",
    "        anomaly_scores.extend(batch_scores)\n",
    "        \n",
    "df_processed['anomaly_score'] = anomaly_scores\n",
    "\n",
    "\n",
    "# 5. 2단계 추론: 최종 이상치(0/1) 예측\n",
    "print(\"2단계 추론을 시작합니다 (최종 예측)...\")\n",
    "features_for_stage2 = ['anomaly_score', 'actor_login_bucket', 'repo_bucket', 'org_bucket']\n",
    "X_test = df_processed[features_for_stage2]\n",
    "\n",
    "# 확률 예측\n",
    "probabilities = model_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 임계값을 기준으로 0 또는 1로 최종 예측\n",
    "df_processed['prediction'] = (probabilities >= THRESHOLD).astype(int)\n",
    "\n",
    "# 6. 결과 저장\n",
    "output_path = os.path.join(DATA_DIR, \"prediction_results.csv\")\n",
    "# 원본 로그의 주요 정보와 예측 결과만 선택하여 저장\n",
    "final_columns = ['id', 'type', 'created_at', 'actor', 'repo', 'org', 'anomaly_score', 'prediction']\n",
    "# df_processed에 없는 컬럼은 제외\n",
    "final_columns_exist = [col for col in final_columns if col in df_processed.columns] \n",
    "\n",
    "df_processed[final_columns_exist].to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"최종 결과가 {output_path} 에 저장되었습니다.\")\n",
    "print(\"각 로그의 마지막 'prediction' 컬럼에서 1이면 이상치, 0이면 정상입니다.\")\n",
    "print(\"\\n--- 탐지 결과 요약 ---\")\n",
    "print(df_processed['prediction'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guardian_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

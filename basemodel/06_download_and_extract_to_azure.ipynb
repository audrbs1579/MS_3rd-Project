{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00f12761",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timezone, timedelta\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# ===================================================================\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# <<<--- 수정된 부분: Spark 세션 초기화 코드 추가 --->>>\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# ===================================================================\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 현재 Spark 세션을 가져오거나, 없으면 새로 생성합니다.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloadAndExtractToAzure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 06) GH Archive 다운로드 및 Azure Blob Storage에 압축 해제하여 저장\n",
    "# \n",
    "# - **워크플로우:**\n",
    "#   1. 지정된 기간의 GH Archive 파일 URL 목록을 생성합니다.\n",
    "#   2. Spark를 사용해 각 URL의 `.json.gz` 파일을 메모리로 다운로드합니다.\n",
    "#   3. 다운로드와 동시에 메모리에서 압축을 해제합니다.\n",
    "#   4. 압축 해제된 `.json` 데이터를 Azure Blob Storage의 지정된 컨테이너/디렉토리에 직접 업로드합니다.\n",
    "# - **의존성:** 클러스터에 `requests`, `azure-storage-blob` 라이브러리가 설치되어 있어야 합니다.\n",
    "\n",
    "# %%\n",
    "# DBTITLE 1,Configuration\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ===================================================================\n",
    "# <<<--- 수정된 부분: Spark 세션 초기화 코드 추가 --->>>\n",
    "# ===================================================================\n",
    "# 현재 Spark 세션을 가져오거나, 없으면 새로 생성합니다.\n",
    "spark = SparkSession.builder.appName(\"DownloadAndExtractToAzure\").getOrCreate()\n",
    "# ===================================================================\n",
    "\n",
    "# ----- 1. Azure Storage 정보 -----\n",
    "AZURE_CONNECTION_STRING = \"DefaultEndpointsProtocol=https;AccountName=mlstorage05team;AccountKey=JmCONXO33L/DymsDfzZzl6XYHmhZu+YQRFa8ISbFZc+GIqAh9dXE0j3ylBg0Vrl2d6BgShpBugFo+AStIaLFlQ==;EndpointSuffix=core.windows.net\"\n",
    "STORAGE_ACCOUNT_NAME = \"mlstorage05team\"\n",
    "CONTAINER_NAME = \"ghachive\"\n",
    "TARGET_DIR = \"raw\"\n",
    "\n",
    "# ----- 2. 다운로드 기간 설정 -----\n",
    "START_DATE = \"2025-08-30\"\n",
    "END_DATE = \"2025-08-31\"\n",
    "\n",
    "# %%\n",
    "# DBTITLE 1,Connect to Azure Storage\n",
    "# Spark 세션에 Azure Storage 계정 키를 설정하여 접근 권한 부여\n",
    "try:\n",
    "    account_key = AZURE_CONNECTION_STRING.split('AccountKey=')[1].split(';')[0]\n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.account.key.{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\",\n",
    "        account_key\n",
    "    )\n",
    "    print(f\"Spark configuration for Azure Storage account '{STORAGE_ACCOUNT_NAME}' is set.\")\n",
    "except IndexError:\n",
    "    raise ValueError(\"Could not parse AccountKey from the connection string.\")\n",
    "\n",
    "# %%\n",
    "# DBTITLE 1,Generate Download & Upload Plan\n",
    "def hour_urls(start_utc, end_utc):\n",
    "    \"\"\"지정된 기간 동안의 GHArchive URL을 생성합니다.\"\"\"\n",
    "    cur = start_utc.replace(minute=0, second=0, microsecond=0)\n",
    "    end = end_utc.replace(minute=0, second=0, microsecond=0)\n",
    "    while cur <= end:\n",
    "        url = f\"https://data.gharchive.org/{cur.strftime('%Y-%m-%d')}-{cur.hour}.json.gz\"\n",
    "        yield cur, url\n",
    "        cur += timedelta(hours=1)\n",
    "\n",
    "# 타임스탬프 생성\n",
    "start_utc = datetime.strptime(START_DATE, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "end_utc = datetime.strptime(END_DATE, \"%Y-%m-%d\").replace(hour=23, minute=59, second=59, tzinfo=timezone.utc)\n",
    "\n",
    "# 전체 작업 목록 생성 (소스 URL, 목적지 Blob 경로)\n",
    "tasks = []\n",
    "for hour_dt, url in hour_urls(start_utc, end_utc):\n",
    "    blob_name = f\"{TARGET_DIR}/{hour_dt.strftime('%Y-%m-%d')}-{hour_dt.hour}.json\"\n",
    "    tasks.append({\"source_url\": url, \"dest_blob\": blob_name})\n",
    "\n",
    "tasks_df = spark.createDataFrame(tasks)\n",
    "\n",
    "print(f\"Total {tasks_df.count()} files will be processed.\")\n",
    "tasks_df.display()\n",
    "\n",
    "# %%\n",
    "# DBTITLE 1,Define Distributed ETL Function\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import pandas as pd\n",
    "\n",
    "def process_and_upload(rows):\n",
    "    \"\"\"Spark 작업자(Worker) 노드에서 실행될 함수.\"\"\"\n",
    "    import requests\n",
    "    import gzip\n",
    "    from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "    connection_string = \"DefaultEndpointsProtocol=https;AccountName=mlstorage05team;AccountKey=JmCONXO33L/DymsDfzZzl6XYHmhZu+YQRFa8ISbFZc+GIqAh9dXE0j3ylBg0Vrl2d6BgShpBugFo+AStIaLFlQ==;EndpointSuffix=core.windows.net\"\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    container_client = blob_service_client.get_container_client(\"ghachive\")\n",
    "    \n",
    "    results = []\n",
    "    for row in rows:\n",
    "        url = row[\"source_url\"]\n",
    "        blob_name = row[\"dest_blob\"]\n",
    "        status = \"FAILED\"\n",
    "        try:\n",
    "            response = requests.get(url, stream=True, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            decompressed_data = gzip.decompress(response.content)\n",
    "            container_client.upload_blob(name=blob_name, data=decompressed_data, overwrite=True)\n",
    "            status = \"SUCCESS\"\n",
    "        except Exception as e:\n",
    "            status = f\"FAILED: {str(e)}\"\n",
    "        \n",
    "        results.append({\"source_url\": url, \"status\": status})\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "result_schema = StructType([\n",
    "    StructField(\"source_url\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# %%\n",
    "# DBTITLE 1,Execute Job and Check Results\n",
    "results_df = tasks_df.mapInPandas(process_and_upload, schema=result_schema)\n",
    "display(results_df)\n",
    "\n",
    "# %%\n",
    "# DBTITLE 1,Verify Uploaded Files on Azure\n",
    "print(f\"Verifying files in Azure container '{CONTAINER_NAME}/{TARGET_DIR}'...\")\n",
    "azure_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.blob.core.windows.net/{TARGET_DIR}\"\n",
    "try:\n",
    "    uploaded_files = dbutils.fs.ls(azure_path)\n",
    "    print(f\"Verification successful. Found {len(uploaded_files)} files in the target directory.\")\n",
    "    for f in uploaded_files[:5]:\n",
    "        print(f\" - {f.path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not verify files. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2473de22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guardian_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12953d8b",
   "metadata": {},
   "source": [
    "# 02) 피처 추출 & 정규화\n",
    "- GH Archive raw(.json.gz)에서 이벤트별 피처를 추출 → 원-핫 인코딩 → 결측/스케일 정리.\n",
    "- RobustScaler → MinMaxScaler(0~1).\n",
    "- 산출물: `features.parquet`, `robust_*.joblib`, `minmax_*.joblib`, `features_*.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d07fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_DIR = r\"C:\\Users\\EL040\\Desktop\\MS_3rd-Project\\basemodel\"\n",
    "SPLIT = \"train\"   # \"train\" 또는 \"test\"\n",
    "USE_FILTERED = False  # True면 *.filtered.json.gz만 사용\n",
    "TAG = SPLIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b699c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parse: 100%|█| 84/84 [16:06<00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw feature frame: (11660754, 48)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, gzip\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from common import get_paths, parse_json_gz, extract_features_from_event, build_dataframe, scale_and_save\n",
    "\n",
    "paths = get_paths(BASE_DIR)\n",
    "raw_dir = paths[\"data_train_raw\"] if SPLIT == \"train\" else paths[\"data_test_raw\"]\n",
    "out_dir = paths[\"data_train_feat\"] if SPLIT == \"train\" else paths[\"data_test_feat\"]\n",
    "scaler_dir = paths[\"model_dir\"]\n",
    "\n",
    "files = sorted([os.path.join(raw_dir, f) for f in os.listdir(raw_dir) if f.endswith(\".json.gz\") and (f.endswith(\".filtered.json.gz\") if USE_FILTERED else True)])\n",
    "\n",
    "print(\"Input files:\", len(files))\n",
    "feats = []\n",
    "for fp in tqdm(files, ncols=30, desc=\"parse\"):\n",
    "    for evt in parse_json_gz(fp):\n",
    "        fd = extract_features_from_event(evt)\n",
    "        if fd is not None:\n",
    "            feats.append(fd)\n",
    "\n",
    "df = build_dataframe(feats)\n",
    "print(\"Raw feature frame:\", df.shape)\n",
    "if df.empty:\n",
    "    raise SystemExit(\"No features extracted. Check input files or filters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6cc20a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw features: C:\\Users\\EL040\\Desktop\\MS_3rd-Project\\basemodel\\data\\train\\features_raw.parquet\n",
      "Base scaler not found. Creating and saving new scalers with tag 'base'...\n",
      "Saved base scalers. Scaled shape: (11660754, 48), Num features: 48\n",
      "Saved scaled features: C:\\Users\\EL040\\Desktop\\MS_3rd-Project\\basemodel\\data\\train\\features.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save raw features (optional)\n",
    "raw_feat_path = os.path.join(out_dir, \"features_raw.parquet\")\n",
    "df.to_parquet(raw_feat_path, index=False)\n",
    "print(\"Saved raw features:\", raw_feat_path)\n",
    "\n",
    "from common import scale_and_save, transform_with_scalers, load_scalers\n",
    "\n",
    "# 'base' 태그의 스케일러가 이미 있는지 확인\n",
    "BASE_TAG = \"base\" # 베이스 모델의 스케일러를 식별하기 위한 태그\n",
    "base_scaler_path = os.path.join(scaler_dir, f\"robust_{BASE_TAG}.joblib\")\n",
    "is_base_scaler_exists = os.path.exists(base_scaler_path)\n",
    "\n",
    "if SPLIT == \"train\" and not is_base_scaler_exists:\n",
    "    # Case 1: 최초의 '베이스 모델' 학습용 데이터 처리\n",
    "    # Scaler를 새로 학습하고 'base' 태그로 저장\n",
    "    print(f\"Base scaler not found. Creating and saving new scalers with tag '{BASE_TAG}'...\")\n",
    "    X_scaled, robust, mm, cols = scale_and_save(df, scaler_dir, BASE_TAG)\n",
    "    print(f\"Saved base scalers. Scaled shape: {X_scaled.shape}, Num features: {len(cols)}\")\n",
    "else:\n",
    "    # Case 2: 이미 '베이스 모델'이 존재하고, 새로운 데이터(train 또는 test)를 처리할 때\n",
    "    # 저장된 'base' 스케일러를 불러와 적용\n",
    "    print(f\"Loading existing base scalers (tag: '{BASE_TAG}') to transform data...\")\n",
    "    if not is_base_scaler_exists:\n",
    "        raise SystemExit(f\"ERROR: Base scaler ('{BASE_TAG}') not found. Please run with a large initial training set first.\")\n",
    "    X_scaled, cols = transform_with_scalers(df, scaler_dir, BASE_TAG)\n",
    "    print(f\"Transformation complete. Scaled shape: {X_scaled.shape}, Num features: {len(cols)}\")\n",
    "\n",
    "# Persist scaled features\n",
    "scaled_feat_path = os.path.join(out_dir, \"features.parquet\")\n",
    "pd.DataFrame(X_scaled, columns=cols).to_parquet(scaled_feat_path, index=False)\n",
    "print(\"Saved scaled features:\", scaled_feat_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guardian_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
